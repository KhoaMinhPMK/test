{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pathlib import Path\n",
        "import json\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import multiprocessing\n",
        "from datetime import datetime\n",
        "\n",
        "# Cấu hình GPU\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(f\"🚀 GPU được phát hiện và cấu hình: {len(gpus)} GPU(s)\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"❌ Lỗi cấu hình GPU: {e}\")\n",
        "\n",
        "# Bật Mixed Precision để tối ưu hiệu suất\n",
        "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "tf.keras.mixed_precision.set_global_policy(policy)\n",
        "print(f\"🎯 Mixed Precision enabled: {policy.name}\")\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
        "print(f\"CPU cores: {multiprocessing.cpu_count()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8d4fb86",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    # Đường dẫn dữ liệu - CẬP NHẬT THEO ĐƯỜNG DẪN MỚI\n",
        "    'base_path': 'khoaminh/data',\n",
        "    'datasets': ['Coffee_room_01', 'Coffee_room_02', 'Home_01', 'Home_02'],\n",
        "    \n",
        "    # Tham số hình ảnh\n",
        "    'input_size': (416, 416),      # Kích thước đầu vào\n",
        "    'num_keypoints': 17,           # COCO pose: 17 keypoints\n",
        "    \n",
        "    # Tham số training\n",
        "    'batch_size': 16,\n",
        "    'epochs': 100,\n",
        "    'learning_rate': 1e-4,\n",
        "    'validation_split': 0.2,\n",
        "    \n",
        "    # Tham số loss weights\n",
        "    'lambda_bbox': 10.0,           # Weight cho bbox regression\n",
        "    'lambda_pose': 5.0,            # Weight cho pose keypoints\n",
        "    'lambda_conf': 1.0,            # Weight cho confidence\n",
        "    \n",
        "    # Tham số khác\n",
        "    'max_samples_per_video': 300,\n",
        "    'confidence_threshold': 0.5,\n",
        "    'save_model_path': 'models/',\n",
        "}\n",
        "\n",
        "# COCO Pose keypoints\n",
        "KEYPOINT_NAMES = [\n",
        "    'nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear',\n",
        "    'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',\n",
        "    'left_wrist', 'right_wrist', 'left_hip', 'right_hip',\n",
        "    'left_knee', 'right_knee', 'left_ankle', 'right_ankle'\n",
        "]\n",
        "\n",
        "# Skeleton connections cho vẽ pose\n",
        "SKELETON = [\n",
        "    [16, 14], [14, 12], [17, 15], [15, 13], [12, 13],  # Chân\n",
        "    [6, 12], [7, 13], [6, 7], [6, 8], [7, 9],         # Thân + tay\n",
        "    [8, 10], [9, 11], [2, 3], [1, 2], [1, 3],         # Tay + mắt\n",
        "    [2, 4], [3, 5], [4, 6], [5, 7]                    # Mặt + vai\n",
        "]\n",
        "\n",
        "print(f\"📋 Person Detection + Pose Configuration:\")\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"   {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a539888",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def find_data_files():\n",
        "    \"\"\"Tìm tất cả file video và annotation\"\"\"\n",
        "    base_path = CONFIG['base_path']\n",
        "    datasets = CONFIG['datasets']\n",
        "    \n",
        "    print(f\"🔍 Tìm kiếm dữ liệu trong: {base_path}\")\n",
        "    \n",
        "    matched_data = []\n",
        "    \n",
        "    for dataset_name in datasets:\n",
        "        print(f\"\\n📁 Xử lý dataset: {dataset_name}\")\n",
        "        \n",
        "        # Đường dẫn video và annotation - CẬP NHẬT THEO CẤU TRÚC MỚI\n",
        "        video_dir = f\"{base_path}/{dataset_name}/{dataset_name}/Videos\"\n",
        "        annotation_dir = f\"{base_path}/{dataset_name}/{dataset_name}/Annotation_files_processed\"\n",
        "        \n",
        "        # Tìm files\n",
        "        video_patterns = [\n",
        "            f\"{video_dir}/*.avi\",\n",
        "            f\"{video_dir}/*.mp4\",\n",
        "            f\"{video_dir}/*.mov\",\n",
        "        ]\n",
        "        \n",
        "        video_files = []\n",
        "        for pattern in video_patterns:\n",
        "            video_files.extend(glob.glob(pattern))\n",
        "        \n",
        "        annotation_files = glob.glob(f\"{annotation_dir}/*_with_pose.txt\")\n",
        "        \n",
        "        print(f\"   🎬 Videos: {len(video_files)}\")\n",
        "        print(f\"   📝 Annotations: {len(annotation_files)}\")\n",
        "        \n",
        "        # Ghép video với annotation\n",
        "        for ann_file in annotation_files:\n",
        "            ann_basename = os.path.basename(ann_file)\n",
        "            video_name_from_ann = ann_basename.replace('_with_pose.txt', '')\n",
        "            video_name_expected = video_name_from_ann.replace('_', ' ')\n",
        "            \n",
        "            possible_names = [\n",
        "                f\"{video_name_expected}.avi\",\n",
        "                f\"{video_name_expected}.mp4\",\n",
        "                f\"{video_name_from_ann}.avi\",\n",
        "                f\"{video_name_from_ann}.mp4\",\n",
        "            ]\n",
        "            \n",
        "            matching_videos = []\n",
        "            for video_file in video_files:\n",
        "                video_basename = os.path.basename(video_file)\n",
        "                if video_basename in possible_names:\n",
        "                    matching_videos.append(video_file)\n",
        "            \n",
        "            if matching_videos:\n",
        "                matched_data.append({\n",
        "                    'dataset': dataset_name,\n",
        "                    'video_name': video_name_from_ann,\n",
        "                    'video_path': matching_videos[0],\n",
        "                    'annotation_path': ann_file\n",
        "                })\n",
        "    \n",
        "    print(f\"\\n📊 Tổng kết: {len(matched_data)} cặp video-annotation được tìm thấy\")\n",
        "    return matched_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "890da5cc",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def parse_annotation_file(annotation_path):\n",
        "    \"\"\"Parse file annotation để lấy bbox và pose keypoints\"\"\"\n",
        "    annotations = {}\n",
        "    \n",
        "    try:\n",
        "        with open(annotation_path, 'r') as f:\n",
        "            fall_start = int(f.readline().strip())\n",
        "            fall_end = int(f.readline().strip())\n",
        "            \n",
        "            for line in f:\n",
        "                parts = line.strip().split(',')\n",
        "                if len(parts) >= 6:\n",
        "                    frame_num = int(parts[0])\n",
        "                    label = int(parts[1])\n",
        "                    \n",
        "                    # Bbox coordinates\n",
        "                    bbox = {\n",
        "                        'x1': int(parts[2]),\n",
        "                        'y1': int(parts[3]),\n",
        "                        'x2': int(parts[4]),\n",
        "                        'y2': int(parts[5])\n",
        "                    }\n",
        "                    \n",
        "                    # Parse pose keypoints\n",
        "                    keypoints = []\n",
        "                    for i in range(6, len(parts)):\n",
        "                        if ':' in parts[i]:\n",
        "                            kp_parts = parts[i].split(':')\n",
        "                            if len(kp_parts) == 4:\n",
        "                                x = float(kp_parts[1])\n",
        "                                y = float(kp_parts[2])\n",
        "                                conf = float(kp_parts[3])\n",
        "                                keypoints.extend([x, y, conf])\n",
        "                    \n",
        "                    # Đảm bảo có đủ 17 keypoints (51 values)\n",
        "                    while len(keypoints) < 51:\n",
        "                        keypoints.extend([0, 0, 0])\n",
        "                    keypoints = keypoints[:51]\n",
        "                    \n",
        "                    annotations[frame_num] = {\n",
        "                        'bbox': bbox,\n",
        "                        'keypoints': keypoints,\n",
        "                        'label': label\n",
        "                    }\n",
        "        \n",
        "        return annotations, fall_start, fall_end\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Lỗi parse {annotation_path}: {e}\")\n",
        "        return {}, -1, -1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3606349b",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def normalize_bbox(bbox, img_width, img_height):\n",
        "    \"\"\"Chuẩn hóa bbox về [0,1]\"\"\"\n",
        "    x1, y1, x2, y2 = bbox['x1'], bbox['y1'], bbox['x2'], bbox['y2']\n",
        "    \n",
        "    # Normalize to [0,1]\n",
        "    x1_norm = x1 / img_width\n",
        "    y1_norm = y1 / img_height\n",
        "    x2_norm = x2 / img_width\n",
        "    y2_norm = y2 / img_height\n",
        "    \n",
        "    return [x1_norm, y1_norm, x2_norm, y2_norm]\n",
        "\n",
        "def normalize_keypoints(keypoints, img_width, img_height):\n",
        "    \"\"\"Normalize keypoints coordinates\"\"\"\n",
        "    normalized_kp = []\n",
        "    for i in range(0, len(keypoints), 3):\n",
        "        if i+2 < len(keypoints):\n",
        "            x = keypoints[i] / img_width\n",
        "            y = keypoints[i+1] / img_height\n",
        "            conf = keypoints[i+2]\n",
        "            normalized_kp.extend([x, y, conf])\n",
        "    \n",
        "    return normalized_kp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1851806",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def augment_image_and_targets(image, bbox, keypoints):\n",
        "    \"\"\"Augmentation cho ảnh và targets\"\"\"\n",
        "    # Random brightness\n",
        "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
        "    \n",
        "    # Random contrast\n",
        "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
        "    \n",
        "    # Random saturation\n",
        "    image = tf.image.random_saturation(image, lower=0.8, upper=1.2)\n",
        "    \n",
        "    # Random horizontal flip\n",
        "    if tf.random.uniform([]) > 0.5:\n",
        "        image = tf.image.flip_left_right(image)\n",
        "        # Flip bbox\n",
        "        x1, y1, x2, y2 = bbox[0], bbox[1], bbox[2], bbox[3]\n",
        "        bbox = tf.stack([1.0 - x2, y1, 1.0 - x1, y2])\n",
        "        \n",
        "        # Flip keypoints (swap left-right pairs)\n",
        "        kp_flipped = tf.identity(keypoints)\n",
        "        # Flip x coordinates\n",
        "        for i in range(0, 51, 3):\n",
        "            kp_flipped = tf.tensor_scatter_nd_update(\n",
        "                kp_flipped, [[i]], [1.0 - keypoints[i]]\n",
        "            )\n",
        "        keypoints = kp_flipped\n",
        "    \n",
        "    return image, bbox, keypoints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f9a2429",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def create_person_pose_dataset(data_files):\n",
        "    \"\"\"Tạo dataset cho Person Detection + Pose training\"\"\"\n",
        "    \n",
        "    def data_generator():\n",
        "        print(\"🔄 Tạo Person Detection + Pose dataset...\")\n",
        "        sample_count = 0\n",
        "        \n",
        "        for data_item in tqdm(data_files, desc=\"Processing videos\"):\n",
        "            video_path = data_item['video_path']\n",
        "            annotation_path = data_item['annotation_path']\n",
        "            \n",
        "            # Parse annotations\n",
        "            annotations, fall_start, fall_end = parse_annotation_file(annotation_path)\n",
        "            if not annotations:\n",
        "                continue\n",
        "            \n",
        "            # Mở video\n",
        "            cap = cv2.VideoCapture(video_path)\n",
        "            if not cap.isOpened():\n",
        "                continue\n",
        "            \n",
        "            frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "            frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "            \n",
        "            # Sample frames có annotation\n",
        "            annotated_frames = list(annotations.keys())\n",
        "            max_samples = min(len(annotated_frames), CONFIG['max_samples_per_video'])\n",
        "            sampled_frames = random.sample(annotated_frames, max_samples)\n",
        "            \n",
        "            for frame_num in sampled_frames:\n",
        "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
        "                ret, frame = cap.read()\n",
        "                \n",
        "                if not ret:\n",
        "                    continue\n",
        "                \n",
        "                # Resize frame\n",
        "                frame_resized = cv2.resize(frame, CONFIG['input_size'])\n",
        "                frame_normalized = frame_resized.astype(np.float32) / 255.0\n",
        "                \n",
        "                # Get annotation\n",
        "                ann = annotations[frame_num]\n",
        "                \n",
        "                # Normalize bbox và keypoints\n",
        "                bbox_norm = normalize_bbox(ann['bbox'], frame_width, frame_height)\n",
        "                keypoints_norm = normalize_keypoints(ann['keypoints'], frame_width, frame_height)\n",
        "                \n",
        "                # Confidence (có người = 1.0)\n",
        "                confidence = 1.0\n",
        "                \n",
        "                yield frame_normalized, bbox_norm, keypoints_norm, confidence\n",
        "                sample_count += 1\n",
        "            \n",
        "            cap.release()\n",
        "        \n",
        "        print(f\"✅ Dataset hoàn thành: {sample_count} samples\")\n",
        "    \n",
        "    # Tạo tf.data.Dataset với format phù hợp cho multi-output model\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "        data_generator,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(*CONFIG['input_size'], 3), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(4,), dtype=tf.float32),  # bbox\n",
        "            tf.TensorSpec(shape=(51,), dtype=tf.float32), # keypoints\n",
        "            tf.TensorSpec(shape=(), dtype=tf.float32)     # confidence\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    # Chuyển đổi format cho multi-output model: (x, (y1, y2, y3))\n",
        "    def reformat_data(image, bbox, keypoints, confidence):\n",
        "        return image, {\n",
        "            'bbox_output': bbox,\n",
        "            'pose_output': keypoints, \n",
        "            'conf_output': confidence\n",
        "        }\n",
        "    \n",
        "    dataset = dataset.map(reformat_data)\n",
        "    \n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def create_person_pose_model():\n",
        "    \"\"\"Tạo custom CNN model cho person detection và pose estimation\"\"\"\n",
        "    \n",
        "    inputs = keras.Input(shape=(*CONFIG['input_size'], 3))\n",
        "    \n",
        "    # Backbone CNN\n",
        "    x = layers.Conv2D(64, 7, strides=2, padding='same', activation='relu')(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
        "    \n",
        "    # ResNet-like blocks\n",
        "    for filters in [64, 128, 256, 512]:\n",
        "        # Block 1\n",
        "        shortcut = x\n",
        "        x = layers.Conv2D(filters, 3, padding='same', activation='relu')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Conv2D(filters, 3, padding='same')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        \n",
        "        # Adjust shortcut if needed\n",
        "        if shortcut.shape[-1] != filters:\n",
        "            shortcut = layers.Conv2D(filters, 1, padding='same')(shortcut)\n",
        "            shortcut = layers.BatchNormalization()(shortcut)\n",
        "        \n",
        "        x = layers.Add()([x, shortcut])\n",
        "        x = layers.Activation('relu')(x)\n",
        "        x = layers.MaxPooling2D(2)(x)\n",
        "    \n",
        "    # Global features\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(1024, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    \n",
        "    # Multi-task outputs\n",
        "    # 1. Bbox regression (4 values: x1, y1, x2, y2)\n",
        "    bbox_output = layers.Dense(256, activation='relu', name='bbox_dense')(x)\n",
        "    bbox_output = layers.Dense(4, activation='sigmoid', name='bbox_output', dtype='float32')(bbox_output)\n",
        "    \n",
        "    # 2. Pose keypoints (51 values: 17 keypoints * 3)\n",
        "    pose_output = layers.Dense(512, activation='relu', name='pose_dense')(x)\n",
        "    pose_output = layers.Dense(51, activation='sigmoid', name='pose_output', dtype='float32')(pose_output)\n",
        "    \n",
        "    # 3. Confidence score (1 value)\n",
        "    conf_output = layers.Dense(128, activation='relu', name='conf_dense')(x)\n",
        "    conf_output = layers.Dense(1, activation='sigmoid', name='conf_output', dtype='float32')(conf_output)\n",
        "    \n",
        "    model = keras.Model(\n",
        "        inputs=inputs, \n",
        "        outputs=[bbox_output, pose_output, conf_output],\n",
        "        name='PersonPoseDetector'\n",
        "    )\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def bbox_loss(y_true, y_pred):\n",
        "    \"\"\"Smooth L1 loss cho bbox regression\"\"\"\n",
        "    diff = tf.abs(y_true - y_pred)\n",
        "    less_than_one = tf.cast(tf.less(diff, 1.0), tf.float32)\n",
        "    smooth_l1_loss = (less_than_one * 0.5 * diff**2) + (1.0 - less_than_one) * (diff - 0.5)\n",
        "    return tf.reduce_mean(smooth_l1_loss)\n",
        "\n",
        "def pose_loss(y_true, y_pred):\n",
        "    \"\"\"MSE loss cho pose keypoints với confidence weighting\"\"\"\n",
        "    # Extract confidence values (every 3rd element starting from index 2)\n",
        "    confidence_mask = y_true[..., 2::3]  # [batch, 17]\n",
        "    \n",
        "    # Reshape để tính loss\n",
        "    y_true_reshaped = tf.reshape(y_true, [-1, 17, 3])\n",
        "    y_pred_reshaped = tf.reshape(y_pred, [-1, 17, 3])\n",
        "    \n",
        "    # Chỉ tính loss cho keypoints có confidence > 0\n",
        "    valid_mask = tf.cast(confidence_mask > 0, tf.float32)\n",
        "    valid_mask = tf.expand_dims(valid_mask, -1)  # [batch, 17, 1]\n",
        "    \n",
        "    # MSE loss\n",
        "    mse = tf.square(y_true_reshaped - y_pred_reshaped)\n",
        "    weighted_mse = mse * valid_mask\n",
        "    \n",
        "    return tf.reduce_mean(weighted_mse)\n",
        "\n",
        "def confidence_loss(y_true, y_pred):\n",
        "    \"\"\"Binary crossentropy cho confidence\"\"\"\n",
        "    return tf.keras.losses.binary_crossentropy(y_true, y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def train_person_pose_model():\n",
        "    \"\"\"Training Person Detection + Pose model\"\"\"\n",
        "    \n",
        "    print(\"🚀 Bắt đầu training Person Detection + Pose model...\")\n",
        "    \n",
        "    # Tìm dữ liệu\n",
        "    data_files = find_data_files()\n",
        "    if not data_files:\n",
        "        print(\"❌ Không tìm thấy dữ liệu!\")\n",
        "        return None\n",
        "    \n",
        "    # Chia train/validation\n",
        "    train_files, val_files = train_test_split(\n",
        "        data_files, \n",
        "        test_size=CONFIG['validation_split'], \n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    print(f\"📊 Chia dữ liệu: {len(train_files)} train, {len(val_files)} validation\")\n",
        "    \n",
        "    # Tạo datasets\n",
        "    print(\"📦 Tạo datasets...\")\n",
        "    train_dataset_raw = create_person_pose_dataset(train_files)\n",
        "    val_dataset_raw = create_person_pose_dataset(val_files)\n",
        "    \n",
        "    # Tối ưu dataset\n",
        "    train_dataset = train_dataset_raw.shuffle(1000).batch(CONFIG['batch_size']).prefetch(tf.data.AUTOTUNE)\n",
        "    val_dataset = val_dataset_raw.batch(CONFIG['batch_size']).prefetch(tf.data.AUTOTUNE)\n",
        "    \n",
        "    # Tạo model\n",
        "    model = create_person_pose_model()\n",
        "    \n",
        "    # Compile model với custom losses\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=CONFIG['learning_rate']),\n",
        "        loss={\n",
        "            'bbox_output': bbox_loss,\n",
        "            'pose_output': pose_loss,\n",
        "            'conf_output': confidence_loss\n",
        "        },\n",
        "        loss_weights={\n",
        "            'bbox_output': CONFIG['lambda_bbox'],\n",
        "            'pose_output': CONFIG['lambda_pose'],\n",
        "            'conf_output': CONFIG['lambda_conf']\n",
        "        },\n",
        "        metrics={\n",
        "            'bbox_output': 'mae',\n",
        "            'pose_output': 'mae',\n",
        "            'conf_output': 'accuracy'\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    print(\"🏗️ Person Detection + Pose Model architecture:\")\n",
        "    model.summary()\n",
        "    \n",
        "    # Callbacks\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    model_save_path = f\"{CONFIG['save_model_path']}PersonPose_{timestamp}.h5\"\n",
        "    \n",
        "    os.makedirs(CONFIG['save_model_path'], exist_ok=True)\n",
        "    \n",
        "    callbacks = [\n",
        "        keras.callbacks.ModelCheckpoint(\n",
        "            model_save_path,\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True,\n",
        "            mode='min',\n",
        "            verbose=1\n",
        "        ),\n",
        "        keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.7,\n",
        "            patience=8,\n",
        "            min_lr=1e-8,\n",
        "            verbose=1\n",
        "        ),\n",
        "        keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=15,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    # Training\n",
        "    print(\"🔥 Bắt đầu training...\")\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        validation_data=val_dataset,\n",
        "        epochs=CONFIG['epochs'],\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    print(f\"✅ Training hoàn thành! Model đã lưu tại: {model_save_path}\")\n",
        "    \n",
        "    return model, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def draw_pose_skeleton(image, keypoints, confidence_threshold=0.3):\n",
        "    \"\"\"Vẽ skeleton pose lên ảnh\"\"\"\n",
        "    \n",
        "    img_height, img_width = image.shape[:2]\n",
        "    \n",
        "    # Vẽ keypoints\n",
        "    for i in range(0, len(keypoints), 3):\n",
        "        if i+2 < len(keypoints):\n",
        "            x = int(keypoints[i] * img_width)\n",
        "            y = int(keypoints[i+1] * img_height)\n",
        "            conf = keypoints[i+2]\n",
        "            \n",
        "            if conf > confidence_threshold:\n",
        "                cv2.circle(image, (x, y), 4, (0, 255, 0), -1)\n",
        "    \n",
        "    # Vẽ skeleton\n",
        "    for connection in SKELETON:\n",
        "        kp1_idx = (connection[0] - 1) * 3  # COCO index bắt đầu từ 1\n",
        "        kp2_idx = (connection[1] - 1) * 3\n",
        "        \n",
        "        if kp1_idx < len(keypoints) and kp2_idx < len(keypoints):\n",
        "            x1 = int(keypoints[kp1_idx] * img_width)\n",
        "            y1 = int(keypoints[kp1_idx + 1] * img_height)\n",
        "            conf1 = keypoints[kp1_idx + 2]\n",
        "            \n",
        "            x2 = int(keypoints[kp2_idx] * img_width)\n",
        "            y2 = int(keypoints[kp2_idx + 1] * img_height)\n",
        "            conf2 = keypoints[kp2_idx + 2]\n",
        "            \n",
        "            if conf1 > confidence_threshold and conf2 > confidence_threshold:\n",
        "                cv2.line(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "    \n",
        "    return image\n",
        "\n",
        "def predict_and_visualize(model, test_files, num_samples=3):\n",
        "    \"\"\"Predict và visualize kết quả\"\"\"\n",
        "    \n",
        "    print(f\"🧪 Testing Person Detection + Pose với {num_samples} samples...\")\n",
        "    \n",
        "    for data_item in test_files[:num_samples]:\n",
        "        video_path = data_item['video_path']\n",
        "        annotation_path = data_item['annotation_path']\n",
        "        \n",
        "        # Parse annotations\n",
        "        annotations, _, _ = parse_annotation_file(annotation_path)\n",
        "        \n",
        "        # Mở video\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            continue\n",
        "        \n",
        "        # Lấy một frame ngẫu nhiên có annotation\n",
        "        annotated_frames = list(annotations.keys())\n",
        "        if not annotated_frames:\n",
        "            cap.release()\n",
        "            continue\n",
        "            \n",
        "        random_frame = random.choice(annotated_frames)\n",
        "        \n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, random_frame)\n",
        "        ret, frame = cap.read()\n",
        "        \n",
        "        if not ret:\n",
        "            cap.release()\n",
        "            continue\n",
        "        \n",
        "        # Preprocess\n",
        "        frame_resized = cv2.resize(frame, CONFIG['input_size'])\n",
        "        frame_normalized = frame_resized.astype(np.float32) / 255.0\n",
        "        input_data = np.expand_dims(frame_normalized, axis=0)\n",
        "        \n",
        "        # Predict\n",
        "        bbox_pred, pose_pred, conf_pred = model.predict(input_data, verbose=0)\n",
        "        \n",
        "        # Extract predictions\n",
        "        bbox = bbox_pred[0]\n",
        "        pose = pose_pred[0]\n",
        "        confidence = conf_pred[0][0]\n",
        "        \n",
        "        # Convert bbox to pixel coordinates\n",
        "        img_h, img_w = CONFIG['input_size']\n",
        "        x1 = int(bbox[0] * img_w)\n",
        "        y1 = int(bbox[1] * img_h)\n",
        "        x2 = int(bbox[2] * img_w)\n",
        "        y2 = int(bbox[3] * img_h)\n",
        "        \n",
        "        # Visualize\n",
        "        result_img = frame_resized.copy()\n",
        "        \n",
        "        # Vẽ predicted bbox\n",
        "        cv2.rectangle(result_img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "        cv2.putText(result_img, f'Conf: {confidence:.2f}', (x1, y1-10), \n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
        "        \n",
        "        # Vẽ predicted pose\n",
        "        result_img = draw_pose_skeleton(result_img, pose)\n",
        "        \n",
        "        # Ground truth\n",
        "        gt_ann = annotations[random_frame]\n",
        "        frame_width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "        frame_height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "        \n",
        "        # Scale ground truth bbox\n",
        "        gt_bbox = gt_ann['bbox']\n",
        "        gt_x1 = int(gt_bbox['x1'] * img_w / frame_width)\n",
        "        gt_y1 = int(gt_bbox['y1'] * img_h / frame_height)\n",
        "        gt_x2 = int(gt_bbox['x2'] * img_w / frame_width)\n",
        "        gt_y2 = int(gt_bbox['y2'] * img_h / frame_height)\n",
        "        \n",
        "        gt_img = frame_resized.copy()\n",
        "        cv2.rectangle(gt_img, (gt_x1, gt_y1), (gt_x2, gt_y2), (0, 255, 0), 2)\n",
        "        \n",
        "        # Scale ground truth keypoints\n",
        "        gt_keypoints = normalize_keypoints(gt_ann['keypoints'], frame_width, frame_height)\n",
        "        gt_img = draw_pose_skeleton(gt_img, gt_keypoints)\n",
        "        \n",
        "        # Display\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        \n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.imshow(cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB))\n",
        "        plt.title('Original Frame')\n",
        "        plt.axis('off')\n",
        "        \n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.imshow(cv2.cvtColor(gt_img, cv2.COLOR_BGR2RGB))\n",
        "        plt.title('Ground Truth')\n",
        "        plt.axis('off')\n",
        "        \n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.imshow(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB))\n",
        "        plt.title(f'Prediction (Conf: {confidence:.2f})')\n",
        "        plt.axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        cap.release()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def plot_training_history(history):\n",
        "    \"\"\"Vẽ biểu đồ training history\"\"\"\n",
        "    \n",
        "    plt.figure(figsize=(15, 10))\n",
        "    \n",
        "    # Total loss\n",
        "    plt.subplot(2, 3, 1)\n",
        "    plt.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
        "    plt.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
        "    plt.title('Total Loss', fontweight='bold')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Bbox loss\n",
        "    plt.subplot(2, 3, 2)\n",
        "    plt.plot(history.history['bbox_output_loss'], label='Train Bbox Loss', linewidth=2)\n",
        "    plt.plot(history.history['val_bbox_output_loss'], label='Val Bbox Loss', linewidth=2)\n",
        "    plt.title('Bbox Loss', fontweight='bold')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Pose loss\n",
        "    plt.subplot(2, 3, 3)\n",
        "    plt.plot(history.history['pose_output_loss'], label='Train Pose Loss', linewidth=2)\n",
        "    plt.plot(history.history['val_pose_output_loss'], label='Val Pose Loss', linewidth=2)\n",
        "    plt.title('Pose Loss', fontweight='bold')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Confidence loss\n",
        "    plt.subplot(2, 3, 4)\n",
        "    plt.plot(history.history['conf_output_loss'], label='Train Conf Loss', linewidth=2)\n",
        "    plt.plot(history.history['val_conf_output_loss'], label='Val Conf Loss', linewidth=2)\n",
        "    plt.title('Confidence Loss', fontweight='bold')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Bbox MAE\n",
        "    plt.subplot(2, 3, 5)\n",
        "    plt.plot(history.history['bbox_output_mae'], label='Train Bbox MAE', linewidth=2)\n",
        "    plt.plot(history.history['val_bbox_output_mae'], label='Val Bbox MAE', linewidth=2)\n",
        "    plt.title('Bbox MAE', fontweight='bold')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MAE')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Confidence accuracy\n",
        "    plt.subplot(2, 3, 6)\n",
        "    plt.plot(history.history['conf_output_accuracy'], label='Train Conf Acc', linewidth=2)\n",
        "    plt.plot(history.history['val_conf_output_accuracy'], label='Val Conf Acc', linewidth=2)\n",
        "    plt.title('Confidence Accuracy', fontweight='bold')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.suptitle('Person Detection + Pose Training Results', fontsize=16, fontweight='bold', y=1.02)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"🎯 PERSON DETECTION + POSE ESTIMATION\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"🎪 Model CNN tùy chỉnh cho person detection và pose estimation!\")\n",
        "    print(\"📋 Features:\")\n",
        "    print(\"   ✅ Custom CNN architecture\")\n",
        "    print(\"   ✅ Multi-task learning\")\n",
        "    print(\"   ✅ Bbox detection\")\n",
        "    print(\"   ✅ Pose keypoint estimation\")\n",
        "    print(\"   ✅ Confidence scoring\")\n",
        "    print(\"   ✅ End-to-end training\")\n",
        "    \n",
        "    # Training\n",
        "    model, history = train_person_pose_model()\n",
        "    \n",
        "    if model is not None and history is not None:\n",
        "        # Vẽ biểu đồ training\n",
        "        plot_training_history(history)\n",
        "        \n",
        "        # Test model\n",
        "        data_files = find_data_files()\n",
        "        if data_files:\n",
        "            predict_and_visualize(model, data_files, num_samples=3)\n",
        "    \n",
        "    print(\"\\n🎉 Person Detection + Pose training hoàn thành!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
