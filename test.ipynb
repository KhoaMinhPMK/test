{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pathlib import Path\n",
        "import json\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import multiprocessing\n",
        "from datetime import datetime\n",
        "\n",
        "# C·∫•u h√¨nh GPU\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(f\"üöÄ GPU ƒë∆∞·ª£c ph√°t hi·ªán v√† c·∫•u h√¨nh: {len(gpus)} GPU(s)\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"‚ùå L·ªói c·∫•u h√¨nh GPU: {e}\")\n",
        "\n",
        "# B·∫≠t Mixed Precision ƒë·ªÉ t·ªëi ∆∞u hi·ªáu su·∫•t\n",
        "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "tf.keras.mixed_precision.set_global_policy(policy)\n",
        "print(f\"üéØ Mixed Precision enabled: {policy.name}\")\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
        "print(f\"CPU cores: {multiprocessing.cpu_count()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8d4fb86",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    # ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu - C·∫¨P NH·∫¨T THEO ƒê∆Ø·ªúNG D·∫™N M·ªöI\n",
        "    'base_path': 'khoaminh/data',\n",
        "    'datasets': ['Coffee_room_01', 'Coffee_room_02', 'Home_01', 'Home_02'],\n",
        "    \n",
        "    # Tham s·ªë h√¨nh ·∫£nh\n",
        "    'input_size': (416, 416),      # K√≠ch th∆∞·ªõc ƒë·∫ßu v√†o\n",
        "    'num_keypoints': 17,           # COCO pose: 17 keypoints\n",
        "    \n",
        "    # Tham s·ªë training\n",
        "    'batch_size': 16,\n",
        "    'epochs': 100,\n",
        "    'learning_rate': 1e-4,\n",
        "    'validation_split': 0.2,\n",
        "    \n",
        "    # Tham s·ªë loss weights\n",
        "    'lambda_bbox': 10.0,           # Weight cho bbox regression\n",
        "    'lambda_pose': 5.0,            # Weight cho pose keypoints\n",
        "    'lambda_conf': 1.0,            # Weight cho confidence\n",
        "    \n",
        "    # Tham s·ªë kh√°c\n",
        "    'max_samples_per_video': 300,\n",
        "    'confidence_threshold': 0.5,\n",
        "    'save_model_path': 'models/',\n",
        "}\n",
        "\n",
        "# COCO Pose keypoints\n",
        "KEYPOINT_NAMES = [\n",
        "    'nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear',\n",
        "    'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',\n",
        "    'left_wrist', 'right_wrist', 'left_hip', 'right_hip',\n",
        "    'left_knee', 'right_knee', 'left_ankle', 'right_ankle'\n",
        "]\n",
        "\n",
        "# Skeleton connections cho v·∫Ω pose\n",
        "SKELETON = [\n",
        "    [16, 14], [14, 12], [17, 15], [15, 13], [12, 13],  # Ch√¢n\n",
        "    [6, 12], [7, 13], [6, 7], [6, 8], [7, 9],         # Th√¢n + tay\n",
        "    [8, 10], [9, 11], [2, 3], [1, 2], [1, 3],         # Tay + m·∫Øt\n",
        "    [2, 4], [3, 5], [4, 6], [5, 7]                    # M·∫∑t + vai\n",
        "]\n",
        "\n",
        "print(f\"üìã Person Detection + Pose Configuration:\")\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"   {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a539888",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def find_data_files():\n",
        "    \"\"\"T√¨m t·∫•t c·∫£ file video v√† annotation\"\"\"\n",
        "    base_path = CONFIG['base_path']\n",
        "    datasets = CONFIG['datasets']\n",
        "    \n",
        "    print(f\"üîç T√¨m ki·∫øm d·ªØ li·ªáu trong: {base_path}\")\n",
        "    \n",
        "    matched_data = []\n",
        "    \n",
        "    for dataset_name in datasets:\n",
        "        print(f\"\\nüìÅ X·ª≠ l√Ω dataset: {dataset_name}\")\n",
        "        \n",
        "        # ƒê∆∞·ªùng d·∫´n video v√† annotation - C·∫¨P NH·∫¨T THEO C·∫§U TR√öC M·ªöI\n",
        "        video_dir = f\"{base_path}/{dataset_name}/{dataset_name}/Videos\"\n",
        "        annotation_dir = f\"{base_path}/{dataset_name}/{dataset_name}/Annotation_files_processed\"\n",
        "        \n",
        "        # T√¨m files\n",
        "        video_patterns = [\n",
        "            f\"{video_dir}/*.avi\",\n",
        "            f\"{video_dir}/*.mp4\",\n",
        "            f\"{video_dir}/*.mov\",\n",
        "        ]\n",
        "        \n",
        "        video_files = []\n",
        "        for pattern in video_patterns:\n",
        "            video_files.extend(glob.glob(pattern))\n",
        "        \n",
        "        annotation_files = glob.glob(f\"{annotation_dir}/*_with_pose.txt\")\n",
        "        \n",
        "        print(f\"   üé¨ Videos: {len(video_files)}\")\n",
        "        print(f\"   üìù Annotations: {len(annotation_files)}\")\n",
        "        \n",
        "        # Gh√©p video v·ªõi annotation\n",
        "        for ann_file in annotation_files:\n",
        "            ann_basename = os.path.basename(ann_file)\n",
        "            video_name_from_ann = ann_basename.replace('_with_pose.txt', '')\n",
        "            video_name_expected = video_name_from_ann.replace('_', ' ')\n",
        "            \n",
        "            possible_names = [\n",
        "                f\"{video_name_expected}.avi\",\n",
        "                f\"{video_name_expected}.mp4\",\n",
        "                f\"{video_name_from_ann}.avi\",\n",
        "                f\"{video_name_from_ann}.mp4\",\n",
        "            ]\n",
        "            \n",
        "            matching_videos = []\n",
        "            for video_file in video_files:\n",
        "                video_basename = os.path.basename(video_file)\n",
        "                if video_basename in possible_names:\n",
        "                    matching_videos.append(video_file)\n",
        "            \n",
        "            if matching_videos:\n",
        "                matched_data.append({\n",
        "                    'dataset': dataset_name,\n",
        "                    'video_name': video_name_from_ann,\n",
        "                    'video_path': matching_videos[0],\n",
        "                    'annotation_path': ann_file\n",
        "                })\n",
        "    \n",
        "    print(f\"\\nüìä T·ªïng k·∫øt: {len(matched_data)} c·∫∑p video-annotation ƒë∆∞·ª£c t√¨m th·∫•y\")\n",
        "    return matched_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "890da5cc",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def parse_annotation_file(annotation_path):\n",
        "    \"\"\"Parse file annotation ƒë·ªÉ l·∫•y bbox v√† pose keypoints\"\"\"\n",
        "    annotations = {}\n",
        "    \n",
        "    try:\n",
        "        with open(annotation_path, 'r') as f:\n",
        "            fall_start = int(f.readline().strip())\n",
        "            fall_end = int(f.readline().strip())\n",
        "            \n",
        "            for line in f:\n",
        "                parts = line.strip().split(',')\n",
        "                if len(parts) >= 6:\n",
        "                    frame_num = int(parts[0])\n",
        "                    label = int(parts[1])\n",
        "                    \n",
        "                    # Bbox coordinates\n",
        "                    bbox = {\n",
        "                        'x1': int(parts[2]),\n",
        "                        'y1': int(parts[3]),\n",
        "                        'x2': int(parts[4]),\n",
        "                        'y2': int(parts[5])\n",
        "                    }\n",
        "                    \n",
        "                    # Parse pose keypoints\n",
        "                    keypoints = []\n",
        "                    for i in range(6, len(parts)):\n",
        "                        if ':' in parts[i]:\n",
        "                            kp_parts = parts[i].split(':')\n",
        "                            if len(kp_parts) == 4:\n",
        "                                x = float(kp_parts[1])\n",
        "                                y = float(kp_parts[2])\n",
        "                                conf = float(kp_parts[3])\n",
        "                                keypoints.extend([x, y, conf])\n",
        "                    \n",
        "                    # ƒê·∫£m b·∫£o c√≥ ƒë·ªß 17 keypoints (51 values)\n",
        "                    while len(keypoints) < 51:\n",
        "                        keypoints.extend([0, 0, 0])\n",
        "                    keypoints = keypoints[:51]\n",
        "                    \n",
        "                    annotations[frame_num] = {\n",
        "                        'bbox': bbox,\n",
        "                        'keypoints': keypoints,\n",
        "                        'label': label\n",
        "                    }\n",
        "        \n",
        "        return annotations, fall_start, fall_end\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå L·ªói parse {annotation_path}: {e}\")\n",
        "        return {}, -1, -1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3606349b",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def normalize_bbox(bbox, img_width, img_height):\n",
        "    \"\"\"Chu·∫©n h√≥a bbox v·ªÅ [0,1]\"\"\"\n",
        "    x1, y1, x2, y2 = bbox['x1'], bbox['y1'], bbox['x2'], bbox['y2']\n",
        "    \n",
        "    # Normalize to [0,1]\n",
        "    x1_norm = x1 / img_width\n",
        "    y1_norm = y1 / img_height\n",
        "    x2_norm = x2 / img_width\n",
        "    y2_norm = y2 / img_height\n",
        "    \n",
        "    return [x1_norm, y1_norm, x2_norm, y2_norm]\n",
        "\n",
        "def normalize_keypoints(keypoints, img_width, img_height):\n",
        "    \"\"\"Normalize keypoints coordinates\"\"\"\n",
        "    normalized_kp = []\n",
        "    for i in range(0, len(keypoints), 3):\n",
        "        if i+2 < len(keypoints):\n",
        "            x = keypoints[i] / img_width\n",
        "            y = keypoints[i+1] / img_height\n",
        "            conf = keypoints[i+2]\n",
        "            normalized_kp.extend([x, y, conf])\n",
        "    \n",
        "    return normalized_kp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1851806",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def augment_image_and_targets(image, bbox, keypoints):\n",
        "    \"\"\"Augmentation cho ·∫£nh v√† targets\"\"\"\n",
        "    # Random brightness\n",
        "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
        "    \n",
        "    # Random contrast\n",
        "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
        "    \n",
        "    # Random saturation\n",
        "    image = tf.image.random_saturation(image, lower=0.8, upper=1.2)\n",
        "    \n",
        "    # Random horizontal flip\n",
        "    if tf.random.uniform([]) > 0.5:\n",
        "        image = tf.image.flip_left_right(image)\n",
        "        # Flip bbox\n",
        "        x1, y1, x2, y2 = bbox[0], bbox[1], bbox[2], bbox[3]\n",
        "        bbox = tf.stack([1.0 - x2, y1, 1.0 - x1, y2])\n",
        "        \n",
        "        # Flip keypoints (swap left-right pairs)\n",
        "        kp_flipped = tf.identity(keypoints)\n",
        "        # Flip x coordinates\n",
        "        for i in range(0, 51, 3):\n",
        "            kp_flipped = tf.tensor_scatter_nd_update(\n",
        "                kp_flipped, [[i]], [1.0 - keypoints[i]]\n",
        "            )\n",
        "        keypoints = kp_flipped\n",
        "    \n",
        "    return image, bbox, keypoints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f9a2429",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def create_person_pose_dataset(data_files):\n",
        "    \"\"\"T·∫°o dataset cho Person Detection + Pose training\"\"\"\n",
        "    \n",
        "    def data_generator():\n",
        "        print(\"üîÑ T·∫°o Person Detection + Pose dataset...\")\n",
        "        sample_count = 0\n",
        "        \n",
        "        for data_item in tqdm(data_files, desc=\"Processing videos\"):\n",
        "            video_path = data_item['video_path']\n",
        "            annotation_path = data_item['annotation_path']\n",
        "            \n",
        "            # Parse annotations\n",
        "            annotations, fall_start, fall_end = parse_annotation_file(annotation_path)\n",
        "            if not annotations:\n",
        "                continue\n",
        "            \n",
        "            # M·ªü video\n",
        "            cap = cv2.VideoCapture(video_path)\n",
        "            if not cap.isOpened():\n",
        "                continue\n",
        "            \n",
        "            frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "            frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "            \n",
        "            # Sample frames c√≥ annotation\n",
        "            annotated_frames = list(annotations.keys())\n",
        "            max_samples = min(len(annotated_frames), CONFIG['max_samples_per_video'])\n",
        "            sampled_frames = random.sample(annotated_frames, max_samples)\n",
        "            \n",
        "            for frame_num in sampled_frames:\n",
        "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
        "                ret, frame = cap.read()\n",
        "                \n",
        "                if not ret:\n",
        "                    continue\n",
        "                \n",
        "                # Resize frame\n",
        "                frame_resized = cv2.resize(frame, CONFIG['input_size'])\n",
        "                frame_normalized = frame_resized.astype(np.float32) / 255.0\n",
        "                \n",
        "                # Get annotation\n",
        "                ann = annotations[frame_num]\n",
        "                \n",
        "                # Normalize bbox v√† keypoints\n",
        "                bbox_norm = normalize_bbox(ann['bbox'], frame_width, frame_height)\n",
        "                keypoints_norm = normalize_keypoints(ann['keypoints'], frame_width, frame_height)\n",
        "                \n",
        "                # Confidence (c√≥ ng∆∞·ªùi = 1.0)\n",
        "                confidence = 1.0\n",
        "                \n",
        "                yield frame_normalized, bbox_norm, keypoints_norm, confidence\n",
        "                sample_count += 1\n",
        "            \n",
        "            cap.release()\n",
        "        \n",
        "        print(f\"‚úÖ Dataset ho√†n th√†nh: {sample_count} samples\")\n",
        "    \n",
        "    # T·∫°o tf.data.Dataset v·ªõi format ph√π h·ª£p cho multi-output model\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "        data_generator,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(*CONFIG['input_size'], 3), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(4,), dtype=tf.float32),  # bbox\n",
        "            tf.TensorSpec(shape=(51,), dtype=tf.float32), # keypoints\n",
        "            tf.TensorSpec(shape=(), dtype=tf.float32)     # confidence\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    # Chuy·ªÉn ƒë·ªïi format cho multi-output model: (x, (y1, y2, y3))\n",
        "    def reformat_data(image, bbox, keypoints, confidence):\n",
        "        return image, {\n",
        "            'bbox_output': bbox,\n",
        "            'pose_output': keypoints, \n",
        "            'conf_output': confidence\n",
        "        }\n",
        "    \n",
        "    dataset = dataset.map(reformat_data)\n",
        "    \n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def create_person_pose_model():\n",
        "    \"\"\"T·∫°o custom CNN model cho person detection v√† pose estimation\"\"\"\n",
        "    \n",
        "    inputs = keras.Input(shape=(*CONFIG['input_size'], 3))\n",
        "    \n",
        "    # Backbone CNN\n",
        "    x = layers.Conv2D(64, 7, strides=2, padding='same', activation='relu')(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
        "    \n",
        "    # ResNet-like blocks\n",
        "    for filters in [64, 128, 256, 512]:\n",
        "        # Block 1\n",
        "        shortcut = x\n",
        "        x = layers.Conv2D(filters, 3, padding='same', activation='relu')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Conv2D(filters, 3, padding='same')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        \n",
        "        # Adjust shortcut if needed\n",
        "        if shortcut.shape[-1] != filters:\n",
        "            shortcut = layers.Conv2D(filters, 1, padding='same')(shortcut)\n",
        "            shortcut = layers.BatchNormalization()(shortcut)\n",
        "        \n",
        "        x = layers.Add()([x, shortcut])\n",
        "        x = layers.Activation('relu')(x)\n",
        "        x = layers.MaxPooling2D(2)(x)\n",
        "    \n",
        "    # Global features\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(1024, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    \n",
        "    # Multi-task outputs\n",
        "    # 1. Bbox regression (4 values: x1, y1, x2, y2)\n",
        "    bbox_output = layers.Dense(256, activation='relu', name='bbox_dense')(x)\n",
        "    bbox_output = layers.Dense(4, activation='sigmoid', name='bbox_output', dtype='float32')(bbox_output)\n",
        "    \n",
        "    # 2. Pose keypoints (51 values: 17 keypoints * 3)\n",
        "    pose_output = layers.Dense(512, activation='relu', name='pose_dense')(x)\n",
        "    pose_output = layers.Dense(51, activation='sigmoid', name='pose_output', dtype='float32')(pose_output)\n",
        "    \n",
        "    # 3. Confidence score (1 value)\n",
        "    conf_output = layers.Dense(128, activation='relu', name='conf_dense')(x)\n",
        "    conf_output = layers.Dense(1, activation='sigmoid', name='conf_output', dtype='float32')(conf_output)\n",
        "    \n",
        "    model = keras.Model(\n",
        "        inputs=inputs, \n",
        "        outputs=[bbox_output, pose_output, conf_output],\n",
        "        name='PersonPoseDetector'\n",
        "    )\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def bbox_loss(y_true, y_pred):\n",
        "    \"\"\"Smooth L1 loss cho bbox regression\"\"\"\n",
        "    diff = tf.abs(y_true - y_pred)\n",
        "    less_than_one = tf.cast(tf.less(diff, 1.0), tf.float32)\n",
        "    smooth_l1_loss = (less_than_one * 0.5 * diff**2) + (1.0 - less_than_one) * (diff - 0.5)\n",
        "    return tf.reduce_mean(smooth_l1_loss)\n",
        "\n",
        "def pose_loss(y_true, y_pred):\n",
        "    \"\"\"MSE loss cho pose keypoints v·ªõi confidence weighting\"\"\"\n",
        "    # Extract confidence values (every 3rd element starting from index 2)\n",
        "    confidence_mask = y_true[..., 2::3]  # [batch, 17]\n",
        "    \n",
        "    # Reshape ƒë·ªÉ t√≠nh loss\n",
        "    y_true_reshaped = tf.reshape(y_true, [-1, 17, 3])\n",
        "    y_pred_reshaped = tf.reshape(y_pred, [-1, 17, 3])\n",
        "    \n",
        "    # Ch·ªâ t√≠nh loss cho keypoints c√≥ confidence > 0\n",
        "    valid_mask = tf.cast(confidence_mask > 0, tf.float32)\n",
        "    valid_mask = tf.expand_dims(valid_mask, -1)  # [batch, 17, 1]\n",
        "    \n",
        "    # MSE loss\n",
        "    mse = tf.square(y_true_reshaped - y_pred_reshaped)\n",
        "    weighted_mse = mse * valid_mask\n",
        "    \n",
        "    return tf.reduce_mean(weighted_mse)\n",
        "\n",
        "def confidence_loss(y_true, y_pred):\n",
        "    \"\"\"Binary crossentropy cho confidence\"\"\"\n",
        "    return tf.keras.losses.binary_crossentropy(y_true, y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def train_person_pose_model():\n",
        "    \"\"\"Training Person Detection + Pose model\"\"\"\n",
        "    \n",
        "    print(\"üöÄ B·∫Øt ƒë·∫ßu training Person Detection + Pose model...\")\n",
        "    \n",
        "    # T√¨m d·ªØ li·ªáu\n",
        "    data_files = find_data_files()\n",
        "    if not data_files:\n",
        "        print(\"‚ùå Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu!\")\n",
        "        return None\n",
        "    \n",
        "    # Chia train/validation\n",
        "    train_files, val_files = train_test_split(\n",
        "        data_files, \n",
        "        test_size=CONFIG['validation_split'], \n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    print(f\"üìä Chia d·ªØ li·ªáu: {len(train_files)} train, {len(val_files)} validation\")\n",
        "    \n",
        "    # T·∫°o datasets\n",
        "    print(\"üì¶ T·∫°o datasets...\")\n",
        "    train_dataset_raw = create_person_pose_dataset(train_files)\n",
        "    val_dataset_raw = create_person_pose_dataset(val_files)\n",
        "    \n",
        "    # T·ªëi ∆∞u dataset\n",
        "    train_dataset = train_dataset_raw.shuffle(1000).batch(CONFIG['batch_size']).prefetch(tf.data.AUTOTUNE)\n",
        "    val_dataset = val_dataset_raw.batch(CONFIG['batch_size']).prefetch(tf.data.AUTOTUNE)\n",
        "    \n",
        "    # T·∫°o model\n",
        "    model = create_person_pose_model()\n",
        "    \n",
        "    # Compile model v·ªõi custom losses\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=CONFIG['learning_rate']),\n",
        "        loss={\n",
        "            'bbox_output': bbox_loss,\n",
        "            'pose_output': pose_loss,\n",
        "            'conf_output': confidence_loss\n",
        "        },\n",
        "        loss_weights={\n",
        "            'bbox_output': CONFIG['lambda_bbox'],\n",
        "            'pose_output': CONFIG['lambda_pose'],\n",
        "            'conf_output': CONFIG['lambda_conf']\n",
        "        },\n",
        "        metrics={\n",
        "            'bbox_output': 'mae',\n",
        "            'pose_output': 'mae',\n",
        "            'conf_output': 'accuracy'\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    print(\"üèóÔ∏è Person Detection + Pose Model architecture:\")\n",
        "    model.summary()\n",
        "    \n",
        "    # Callbacks\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    model_save_path = f\"{CONFIG['save_model_path']}PersonPose_{timestamp}.h5\"\n",
        "    \n",
        "    os.makedirs(CONFIG['save_model_path'], exist_ok=True)\n",
        "    \n",
        "    callbacks = [\n",
        "        keras.callbacks.ModelCheckpoint(\n",
        "            model_save_path,\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True,\n",
        "            mode='min',\n",
        "            verbose=1\n",
        "        ),\n",
        "        keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.7,\n",
        "            patience=8,\n",
        "            min_lr=1e-8,\n",
        "            verbose=1\n",
        "        ),\n",
        "        keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=15,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    # Training\n",
        "    print(\"üî• B·∫Øt ƒë·∫ßu training...\")\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        validation_data=val_dataset,\n",
        "        epochs=CONFIG['epochs'],\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úÖ Training ho√†n th√†nh! Model ƒë√£ l∆∞u t·∫°i: {model_save_path}\")\n",
        "    \n",
        "    return model, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def draw_pose_skeleton(image, keypoints, confidence_threshold=0.3):\n",
        "    \"\"\"V·∫Ω skeleton pose l√™n ·∫£nh\"\"\"\n",
        "    \n",
        "    img_height, img_width = image.shape[:2]\n",
        "    \n",
        "    # V·∫Ω keypoints\n",
        "    for i in range(0, len(keypoints), 3):\n",
        "        if i+2 < len(keypoints):\n",
        "            x = int(keypoints[i] * img_width)\n",
        "            y = int(keypoints[i+1] * img_height)\n",
        "            conf = keypoints[i+2]\n",
        "            \n",
        "            if conf > confidence_threshold:\n",
        "                cv2.circle(image, (x, y), 4, (0, 255, 0), -1)\n",
        "    \n",
        "    # V·∫Ω skeleton\n",
        "    for connection in SKELETON:\n",
        "        kp1_idx = (connection[0] - 1) * 3  # COCO index b·∫Øt ƒë·∫ßu t·ª´ 1\n",
        "        kp2_idx = (connection[1] - 1) * 3\n",
        "        \n",
        "        if kp1_idx < len(keypoints) and kp2_idx < len(keypoints):\n",
        "            x1 = int(keypoints[kp1_idx] * img_width)\n",
        "            y1 = int(keypoints[kp1_idx + 1] * img_height)\n",
        "            conf1 = keypoints[kp1_idx + 2]\n",
        "            \n",
        "            x2 = int(keypoints[kp2_idx] * img_width)\n",
        "            y2 = int(keypoints[kp2_idx + 1] * img_height)\n",
        "            conf2 = keypoints[kp2_idx + 2]\n",
        "            \n",
        "            if conf1 > confidence_threshold and conf2 > confidence_threshold:\n",
        "                cv2.line(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "    \n",
        "    return image\n",
        "\n",
        "def predict_and_visualize(model, test_files, num_samples=3):\n",
        "    \"\"\"Predict v√† visualize k·∫øt qu·∫£\"\"\"\n",
        "    \n",
        "    print(f\"üß™ Testing Person Detection + Pose v·ªõi {num_samples} samples...\")\n",
        "    \n",
        "    for data_item in test_files[:num_samples]:\n",
        "        video_path = data_item['video_path']\n",
        "        annotation_path = data_item['annotation_path']\n",
        "        \n",
        "        # Parse annotations\n",
        "        annotations, _, _ = parse_annotation_file(annotation_path)\n",
        "        \n",
        "        # M·ªü video\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            continue\n",
        "        \n",
        "        # L·∫•y m·ªôt frame ng·∫´u nhi√™n c√≥ annotation\n",
        "        annotated_frames = list(annotations.keys())\n",
        "        if not annotated_frames:\n",
        "            cap.release()\n",
        "            continue\n",
        "            \n",
        "        random_frame = random.choice(annotated_frames)\n",
        "        \n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, random_frame)\n",
        "        ret, frame = cap.read()\n",
        "        \n",
        "        if not ret:\n",
        "            cap.release()\n",
        "            continue\n",
        "        \n",
        "        # Preprocess\n",
        "        frame_resized = cv2.resize(frame, CONFIG['input_size'])\n",
        "        frame_normalized = frame_resized.astype(np.float32) / 255.0\n",
        "        input_data = np.expand_dims(frame_normalized, axis=0)\n",
        "        \n",
        "        # Predict\n",
        "        bbox_pred, pose_pred, conf_pred = model.predict(input_data, verbose=0)\n",
        "        \n",
        "        # Extract predictions\n",
        "        bbox = bbox_pred[0]\n",
        "        pose = pose_pred[0]\n",
        "        confidence = conf_pred[0][0]\n",
        "        \n",
        "        # Convert bbox to pixel coordinates\n",
        "        img_h, img_w = CONFIG['input_size']\n",
        "        x1 = int(bbox[0] * img_w)\n",
        "        y1 = int(bbox[1] * img_h)\n",
        "        x2 = int(bbox[2] * img_w)\n",
        "        y2 = int(bbox[3] * img_h)\n",
        "        \n",
        "        # Visualize\n",
        "        result_img = frame_resized.copy()\n",
        "        \n",
        "        # V·∫Ω predicted bbox\n",
        "        cv2.rectangle(result_img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "        cv2.putText(result_img, f'Conf: {confidence:.2f}', (x1, y1-10), \n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
        "        \n",
        "        # V·∫Ω predicted pose\n",
        "        result_img = draw_pose_skeleton(result_img, pose)\n",
        "        \n",
        "        # Ground truth\n",
        "        gt_ann = annotations[random_frame]\n",
        "        frame_width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "        frame_height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "        \n",
        "        # Scale ground truth bbox\n",
        "        gt_bbox = gt_ann['bbox']\n",
        "        gt_x1 = int(gt_bbox['x1'] * img_w / frame_width)\n",
        "        gt_y1 = int(gt_bbox['y1'] * img_h / frame_height)\n",
        "        gt_x2 = int(gt_bbox['x2'] * img_w / frame_width)\n",
        "        gt_y2 = int(gt_bbox['y2'] * img_h / frame_height)\n",
        "        \n",
        "        gt_img = frame_resized.copy()\n",
        "        cv2.rectangle(gt_img, (gt_x1, gt_y1), (gt_x2, gt_y2), (0, 255, 0), 2)\n",
        "        \n",
        "        # Scale ground truth keypoints\n",
        "        gt_keypoints = normalize_keypoints(gt_ann['keypoints'], frame_width, frame_height)\n",
        "        gt_img = draw_pose_skeleton(gt_img, gt_keypoints)\n",
        "        \n",
        "        # Display\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        \n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.imshow(cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB))\n",
        "        plt.title('Original Frame')\n",
        "        plt.axis('off')\n",
        "        \n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.imshow(cv2.cvtColor(gt_img, cv2.COLOR_BGR2RGB))\n",
        "        plt.title('Ground Truth')\n",
        "        plt.axis('off')\n",
        "        \n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.imshow(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB))\n",
        "        plt.title(f'Prediction (Conf: {confidence:.2f})')\n",
        "        plt.axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        cap.release()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def plot_training_history(history):\n",
        "    \"\"\"V·∫Ω bi·ªÉu ƒë·ªì training history\"\"\"\n",
        "    \n",
        "    plt.figure(figsize=(15, 10))\n",
        "    \n",
        "    # Total loss\n",
        "    plt.subplot(2, 3, 1)\n",
        "    plt.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
        "    plt.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
        "    plt.title('Total Loss', fontweight='bold')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Bbox loss\n",
        "    plt.subplot(2, 3, 2)\n",
        "    plt.plot(history.history['bbox_output_loss'], label='Train Bbox Loss', linewidth=2)\n",
        "    plt.plot(history.history['val_bbox_output_loss'], label='Val Bbox Loss', linewidth=2)\n",
        "    plt.title('Bbox Loss', fontweight='bold')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Pose loss\n",
        "    plt.subplot(2, 3, 3)\n",
        "    plt.plot(history.history['pose_output_loss'], label='Train Pose Loss', linewidth=2)\n",
        "    plt.plot(history.history['val_pose_output_loss'], label='Val Pose Loss', linewidth=2)\n",
        "    plt.title('Pose Loss', fontweight='bold')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Confidence loss\n",
        "    plt.subplot(2, 3, 4)\n",
        "    plt.plot(history.history['conf_output_loss'], label='Train Conf Loss', linewidth=2)\n",
        "    plt.plot(history.history['val_conf_output_loss'], label='Val Conf Loss', linewidth=2)\n",
        "    plt.title('Confidence Loss', fontweight='bold')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Bbox MAE\n",
        "    plt.subplot(2, 3, 5)\n",
        "    plt.plot(history.history['bbox_output_mae'], label='Train Bbox MAE', linewidth=2)\n",
        "    plt.plot(history.history['val_bbox_output_mae'], label='Val Bbox MAE', linewidth=2)\n",
        "    plt.title('Bbox MAE', fontweight='bold')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MAE')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Confidence accuracy\n",
        "    plt.subplot(2, 3, 6)\n",
        "    plt.plot(history.history['conf_output_accuracy'], label='Train Conf Acc', linewidth=2)\n",
        "    plt.plot(history.history['val_conf_output_accuracy'], label='Val Conf Acc', linewidth=2)\n",
        "    plt.title('Confidence Accuracy', fontweight='bold')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.suptitle('Person Detection + Pose Training Results', fontsize=16, fontweight='bold', y=1.02)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"üéØ PERSON DETECTION + POSE ESTIMATION\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"üé™ Model CNN t√πy ch·ªânh cho person detection v√† pose estimation!\")\n",
        "    print(\"üìã Features:\")\n",
        "    print(\"   ‚úÖ Custom CNN architecture\")\n",
        "    print(\"   ‚úÖ Multi-task learning\")\n",
        "    print(\"   ‚úÖ Bbox detection\")\n",
        "    print(\"   ‚úÖ Pose keypoint estimation\")\n",
        "    print(\"   ‚úÖ Confidence scoring\")\n",
        "    print(\"   ‚úÖ End-to-end training\")\n",
        "    \n",
        "    # Training\n",
        "    model, history = train_person_pose_model()\n",
        "    \n",
        "    if model is not None and history is not None:\n",
        "        # V·∫Ω bi·ªÉu ƒë·ªì training\n",
        "        plot_training_history(history)\n",
        "        \n",
        "        # Test model\n",
        "        data_files = find_data_files()\n",
        "        if data_files:\n",
        "            predict_and_visualize(model, data_files, num_samples=3)\n",
        "    \n",
        "    print(\"\\nüéâ Person Detection + Pose training ho√†n th√†nh!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
