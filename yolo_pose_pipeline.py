#%% YOLO Pose Detection Pipeline ƒë·ªÉ t·∫°o Dataset m·ªõi
import os
import glob
import cv2
import numpy as np
from ultralytics import YOLO
import json
from tqdm import tqdm
from pathlib import Path

print("üöÄ YOLO POSE DETECTION PIPELINE")
print("=" * 50)
print("üìã M·ª•c ti√™u:")
print("   ‚úÖ S·ª≠ d·ª•ng YOLO detect ng∆∞·ªùi v√† pose")
print("   ‚úÖ ƒê·ªçc nh√£n c≈© l·∫•y th·ªùi gian ng√£")
print("   ‚úÖ T·∫°o file nh√£n m·ªõi v·ªõi YOLO detection")
print("   ‚úÖ X·ª≠ l√Ω t·∫•t c·∫£ video trong dataset")

#%% C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n
CONFIG = {
    # ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu - GI·ªÆ NGUY√äN THEO Y√äU C·∫¶U
    'datasets': [
        {
            'name': 'Coffee_room_01',
            'video_path': 'khoaminh/data/Coffee_room_01/Coffee_room_01/Videos',
            'annotation_path': 'khoaminh/data/Coffee_room_01/Coffee_room_01/Annotation_files_processed'
        },
        {
            'name': 'Coffee_room_02', 
            'video_path': 'khoaminh/data/Coffee_room_02/Coffee_room_02/Videos',
            'annotation_path': 'khoaminh/data/Coffee_room_02/Coffee_room_02/Annotation_files_processed'
        },
        {
            'name': 'Home_01',
            'video_path': 'khoaminh/data/Home_01/Home_01/Videos', 
            'annotation_path': 'khoaminh/data/Home_01/Home_01/Annotation_files_processed'
        },
        {
            'name': 'Home_02',
            'video_path': 'khoaminh/data/Home_02/Home_02/Videos',
            'annotation_path': 'khoaminh/data/Home_02/Home_02/Annotation_files_processed'
        }
    ],
    
    # ƒê∆∞·ªùng d·∫´n output
    'output_dir': 'new_dataset',
    
    # YOLO model
    'yolo_model': 'yolo11x-pose.pt',  # YOLOv8 Pose model
    
    # Tham s·ªë detection
    'confidence_threshold': 0.5,
    'person_class_id': 0,  # Class ID cho person trong COCO
    
    # Tham s·ªë kh√°c
    'max_frames_per_video': None,  # None = x·ª≠ l√Ω t·∫•t c·∫£ frames
}

print(f"\nüìÅ S·∫Ω x·ª≠ l√Ω {len(CONFIG['datasets'])} datasets:")
for dataset in CONFIG['datasets']:
    print(f"   üìÇ {dataset['name']}")

#%% Load YOLO model
def load_yolo_model():
    """Load YOLO Pose model"""
    print(f"\nü§ñ Loading YOLO model: {CONFIG['yolo_model']}")
    try:
        model = YOLO(CONFIG['yolo_model'])
        print("‚úÖ YOLO model loaded successfully!")
        return model
    except Exception as e:
        print(f"‚ùå L·ªói load YOLO model: {e}")
        return None

#%% ƒê·ªçc nh√£n c≈© ƒë·ªÉ l·∫•y th·ªùi gian ng√£
def read_old_annotation(annotation_file):
    """ƒê·ªçc file annotation c≈© ƒë·ªÉ l·∫•y th·ªùi gian ng√£"""
    try:
        with open(annotation_file, 'r') as f:
            lines = f.readlines()
            
        if len(lines) < 2:
            print(f"‚ùå File annotation kh√¥ng ƒë·ªß d·ªØ li·ªáu: {annotation_file}")
            return None, None
            
        # D√≤ng ƒë·∫ßu ti√™n: frame b·∫Øt ƒë·∫ßu ng√£
        fall_start = int(lines[0].strip())
        # D√≤ng th·ª© hai: frame k·∫øt th√∫c ng√£  
        fall_end = int(lines[1].strip())
        
        print(f"üìä Th·ªùi gian ng√£: frame {fall_start} -> {fall_end}")
        return fall_start, fall_end
        
    except Exception as e:
        print(f"‚ùå L·ªói ƒë·ªçc annotation {annotation_file}: {e}")
        return None, None

#%% YOLO Detection v√† Pose Estimation
def detect_person_pose(model, frame):
    """Detect ng∆∞·ªùi v√† pose keypoints b·∫±ng YOLO"""
    try:
        # Run YOLO inference
        results = model(frame, conf=CONFIG['confidence_threshold'], verbose=False)
        
        detections = []
        
        for result in results:
            # L·∫•y boxes v√† keypoints
            if result.boxes is not None and result.keypoints is not None:
                boxes = result.boxes.data.cpu().numpy()
                keypoints = result.keypoints.data.cpu().numpy()
                
                for i, box in enumerate(boxes):
                    # Ch·ªâ l·∫•y detection c·ªßa person (class 0)
                    if int(box[5]) == CONFIG['person_class_id']:
                        # Bbox coordinates
                        x1, y1, x2, y2 = box[:4]
                        confidence = box[4]
                        
                        # Keypoints (17 keypoints, m·ªói keypoint c√≥ x, y, confidence)
                        if i < len(keypoints):
                            kp = keypoints[i]  # Shape: (17, 3)
                            
                            detection = {
                                'bbox': [float(x1), float(y1), float(x2), float(y2)],
                                'confidence': float(confidence),
                                'keypoints': kp.flatten().tolist()  # Flatten to 51 values
                            }
                            detections.append(detection)
        
        return detections
        
    except Exception as e:
        print(f"‚ùå L·ªói YOLO detection: {e}")
        return []

#%% T√¨m video v√† annotation t∆∞∆°ng ·ª©ng
def find_matching_files(dataset_config):
    """T√¨m c√°c c·∫∑p video-annotation t∆∞∆°ng ·ª©ng"""
    video_dir = dataset_config['video_path']
    annotation_dir = dataset_config['annotation_path']
    
    print(f"\nüîç T√¨m ki·∫øm files trong {dataset_config['name']}:")
    print(f"   üìπ Video dir: {video_dir}")
    print(f"   üìù Annotation dir: {annotation_dir}")
    
    # T√¨m video files
    video_extensions = ['*.avi', '*.mp4', '*.mov', '*.mkv']
    video_files = []
    for ext in video_extensions:
        video_files.extend(glob.glob(os.path.join(video_dir, ext)))
    
    # T√¨m annotation files
    annotation_files = glob.glob(os.path.join(annotation_dir, "*_with_pose.txt"))
    
    print(f"   üìä T√¨m th·∫•y: {len(video_files)} videos, {len(annotation_files)} annotations")
    
    # Gh√©p video v·ªõi annotation
    matched_pairs = []
    for ann_file in annotation_files:
        ann_basename = os.path.basename(ann_file)
        # Lo·∫°i b·ªè "_with_pose.txt" ƒë·ªÉ l·∫•y t√™n video
        video_name_base = ann_basename.replace('_with_pose.txt', '')
        
        # Th·ª≠ c√°c t√™n video c√≥ th·ªÉ c√≥
        possible_video_names = [
            video_name_base.replace('_', ' ') + '.avi',
            video_name_base.replace('_', ' ') + '.mp4',
            video_name_base + '.avi',
            video_name_base + '.mp4',
        ]
        
        matching_video = None
        for video_file in video_files:
            video_basename = os.path.basename(video_file)
            if video_basename in possible_video_names:
                matching_video = video_file
                break
        
        if matching_video:
            matched_pairs.append({
                'video_path': matching_video,
                'annotation_path': ann_file,
                'video_name': video_name_base
            })
            print(f"   ‚úÖ Matched: {os.path.basename(matching_video)} <-> {ann_basename}")
        else:
            print(f"   ‚ùå No video found for: {ann_basename}")
    
    print(f"   üìã T·ªïng c·ªông: {len(matched_pairs)} c·∫∑p video-annotation")
    return matched_pairs

#%% X·ª≠ l√Ω m·ªôt video
def process_video(model, video_info, dataset_name, output_dir):
    """X·ª≠ l√Ω m·ªôt video ƒë·ªÉ t·∫°o nh√£n m·ªõi"""
    video_path = video_info['video_path']
    annotation_path = video_info['annotation_path']
    video_name = video_info['video_name']
    
    print(f"\nüé¨ X·ª≠ l√Ω video: {os.path.basename(video_path)}")
    
    # ƒê·ªçc th·ªùi gian ng√£ t·ª´ annotation c≈©
    fall_start, fall_end = read_old_annotation(annotation_path)
    if fall_start is None or fall_end is None:
        print(f"‚ùå Kh√¥ng th·ªÉ ƒë·ªçc th·ªùi gian ng√£, b·ªè qua video n√†y")
        return False
    
    # M·ªü video
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"‚ùå Kh√¥ng th·ªÉ m·ªü video: {video_path}")
        return False
    
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    
    print(f"üìä Video info: {total_frames} frames, {fps:.2f} FPS")
    
    # T·∫°o th∆∞ m·ª•c output
    dataset_output_dir = os.path.join(output_dir, dataset_name)
    os.makedirs(dataset_output_dir, exist_ok=True)
    
    # T√™n file output
    output_file = os.path.join(dataset_output_dir, f"{video_name}_yolo_pose.txt")
    
    # X·ª≠ l√Ω t·ª´ng frame
    frame_detections = {}
    frame_count = 0
    
    max_frames = CONFIG['max_frames_per_video'] or total_frames
    
    with tqdm(total=min(total_frames, max_frames), desc="Processing frames") as pbar:
        while True:
            ret, frame = cap.read()
            if not ret or frame_count >= max_frames:
                break
            
            # YOLO detection
            detections = detect_person_pose(model, frame)
            
            if detections:
                # L·∫•y detection t·ªët nh·∫•t (confidence cao nh·∫•t)
                best_detection = max(detections, key=lambda x: x['confidence'])
                frame_detections[frame_count] = best_detection
            
            frame_count += 1
            pbar.update(1)
    
    cap.release()
    
    # Ghi file nh√£n m·ªõi
    write_new_annotation(output_file, frame_detections, fall_start, fall_end, total_frames)
    
    print(f"‚úÖ Ho√†n th√†nh: {len(frame_detections)}/{total_frames} frames c√≥ detection")
    print(f"üìä T·∫•t c·∫£ {total_frames} frames ƒë√£ ƒë∆∞·ª£c ghi ƒë·ªÉ kh·ªõp v·ªõi data c≈©")
    print(f"üíæ Saved: {output_file}")
    
    return True

#%% Ghi file annotation m·ªõi
def write_new_annotation(output_file, frame_detections, fall_start, fall_end, total_frames):
    """Ghi file annotation m·ªõi v·ªõi format chu·∫©n"""
    
    with open(output_file, 'w') as f:
        # D√≤ng 1: Frame b·∫Øt ƒë·∫ßu ng√£
        f.write(f"{fall_start}\n")
        
        # D√≤ng 2: Frame k·∫øt th√∫c ng√£
        f.write(f"{fall_end}\n")
        
        # C√°c d√≤ng ti·∫øp theo: GHI T·∫§T C·∫¢ FRAMES ƒë·ªÉ kh·ªõp v·ªõi data c≈©
        for frame_num in range(total_frames):
            # Label ƒë∆°n gi·∫£n: ch·ªâ ph√¢n bi·ªát ng√£ hay kh√¥ng ng√£
            if fall_start <= frame_num <= fall_end:
                label = 1  # Falling (ƒëang ng√£)
            else:
                label = 0  # Normal (kh√¥ng ng√£)
            
            if frame_num in frame_detections:
                # Frame c√≥ detection
                detection = frame_detections[frame_num]
                
                # Bbox coordinates
                x1, y1, x2, y2 = detection['bbox']
                
                # Keypoints (51 values: 17 keypoints * 3 (x, y, conf))
                keypoints = detection['keypoints']
                
                # Format keypoints as string
                kp_str = ""
                for i in range(0, len(keypoints), 3):
                    if i + 2 < len(keypoints):
                        x, y, conf = keypoints[i], keypoints[i+1], keypoints[i+2]
                        kp_str += f"kp{i//3}:{x:.2f}:{y:.2f}:{conf:.2f},"
                
                # Lo·∫°i b·ªè d·∫•u ph·∫©y cu·ªëi
                kp_str = kp_str.rstrip(',')
                
                # Ghi d√≤ng v·ªõi detection
                line = f"{frame_num},{label},{int(x1)},{int(y1)},{int(x2)},{int(y2)},{kp_str}\n"
                f.write(line)
            else:
                # Frame KH√îNG c√≥ detection - ghi v·ªõi bbox v√† keypoints = 0
                # T·∫°o keypoints r·ªóng (17 keypoints * 3 = 51 values, t·∫•t c·∫£ = 0)
                empty_kp_str = ",".join([f"kp{i}:0.00:0.00:0.00" for i in range(17)])
                
                # Ghi d√≤ng v·ªõi bbox = 0 v√† keypoints = 0
                line = f"{frame_num},{label},0,0,0,0,{empty_kp_str}\n"
                f.write(line)

#%% Main pipeline
def main():
    """Main pipeline function"""
    print("\nüöÄ B·∫Øt ƒë·∫ßu YOLO Pose Detection Pipeline...")
    
    # Load YOLO model
    model = load_yolo_model()
    if model is None:
        print("‚ùå Kh√¥ng th·ªÉ load YOLO model, d·ª´ng pipeline")
        return
    
    # T·∫°o th∆∞ m·ª•c output
    output_dir = CONFIG['output_dir']
    os.makedirs(output_dir, exist_ok=True)
    print(f"üìÅ Output directory: {output_dir}")
    
    # X·ª≠ l√Ω t·ª´ng dataset
    total_processed = 0
    total_failed = 0
    
    for dataset_config in CONFIG['datasets']:
        print(f"\n{'='*60}")
        print(f"üìÇ X·ª≠ l√Ω dataset: {dataset_config['name']}")
        print(f"{'='*60}")
        
        # T√¨m c√°c c·∫∑p video-annotation
        matched_pairs = find_matching_files(dataset_config)
        
        if not matched_pairs:
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y c·∫∑p video-annotation n√†o trong {dataset_config['name']}")
            continue
        
        # X·ª≠ l√Ω t·ª´ng video
        for video_info in matched_pairs:
            try:
                success = process_video(model, video_info, dataset_config['name'], output_dir)
                if success:
                    total_processed += 1
                else:
                    total_failed += 1
            except Exception as e:
                print(f"‚ùå L·ªói x·ª≠ l√Ω video {video_info['video_name']}: {e}")
                total_failed += 1
    
    # T·ªïng k·∫øt
    print(f"\n{'='*60}")
    print(f"üéâ PIPELINE HO√ÄN TH√ÄNH!")
    print(f"{'='*60}")
    print(f"‚úÖ Th√†nh c√¥ng: {total_processed} videos")
    print(f"‚ùå Th·∫•t b·∫°i: {total_failed} videos")
    print(f"üìÅ Output directory: {output_dir}")
    print(f"üìã Format file m·ªõi: frame_start, frame_end, frame_data...")
    print(f"üìã Label mapping: 0=Normal, 1=Falling")
    print(f"üìã Ch·ªâ l·∫•y th·ªùi gian ng√£ t·ª´ data c≈©, logic ph√¢n lo·∫°i ƒë∆°n gi·∫£n!")
    print(f"üìã GHI T·∫§T C·∫¢ FRAMES (k·ªÉ c·∫£ kh√¥ng c√≥ detection) ƒë·ªÉ kh·ªõp v·ªõi data c≈©!")

if __name__ == "__main__":
    main() 